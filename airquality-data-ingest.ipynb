{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIAZL7Q3WTIK4SQKPEK\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import requests\n",
    "import os\n",
    "from requests_kerberos import HTTPKerberosAuth\n",
    "\n",
    "IDBROKER_HOST_NAME = \"go01-aw-dl-md-idbroker1.go01-dem.ylcu-atmi.cloudera.site\"\n",
    "IDBROKER_HOST_PORT = \"8444\"\n",
    "# S3_BUCKET ='go01-demo'\n",
    "token_url = f\"https://{IDBROKER_HOST_NAME}:{IDBROKER_HOST_PORT}/gateway/dt/knoxtoken/api/v1/token\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "url =  f\"https://{IDBROKER_HOST_NAME}:{IDBROKER_HOST_PORT}/gateway/aws-cab/cab/api/v1/credentials\"\n",
    "r = requests.get(token_url, auth=HTTPKerberosAuth())\n",
    "\n",
    "\n",
    "headers = {\n",
    "    'Authorization': \"Bearer \"+ r.json()['access_token'],\n",
    "    'cache-control': \"no-cache\"\n",
    "    }\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers)\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]=response.json()['Credentials']['AccessKeyId']\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]=response.json()['Credentials']['SecretAccessKey']\n",
    "os.environ[\"AWS_DEFAULT_REGION\"]='us-east-2'\n",
    "os.environ[\"AWS_SESSION_TOKEN\"]=response.json()['Credentials']['SessionToken']\n",
    "print(os.environ[\"AWS_ACCESS_KEY_ID\"])\n",
    "\n",
    "\n",
    "ACCESS_KEY=os.environ[\"AWS_ACCESS_KEY_ID\"]\n",
    "SECRET_KEY=os.environ[\"AWS_SECRET_ACCESS_KEY\"]\n",
    "SESSION_TOKEN=os.environ[\"AWS_SESSION_TOKEN\"]\n",
    "REGION_NAME=os.environ[\"AWS_DEFAULT_REGION\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/cdsw\")\n",
    "from utils.access_keys import ACCESS_KEY, SECRET_KEY, SESSION_TOKEN, REGION_NAME\n",
    "\n",
    "import boto3\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "\n",
    "# Configuration\n",
    "CITIES_CONFIG_FILE = \"cities_config.json\"  # JSON file with city names and bounding boxes\n",
    "SOURCE_BUCKET_NAME = \"openaq-data-archive\"\n",
    "TARGET_BUCKET_NAME = \"go01-demo\"  # Replace with your destination bucket\n",
    "REGION_NAME = \"us-east-1\"\n",
    "OUTPUT_FILE_KEY_PREFIX = \"user/vishr/data/airquality\"\n",
    "API_KEY =  \"4b403c90de7f6d5fdc133c53c04ac66b4063c7c0f90d908784321d5226a59394\"  # Replace with your actual OpenAQ API key\n",
    "\n",
    "# Initialize sessions\n",
    "anonymous_session = boto3.Session()  # For public bucket\n",
    "authenticated_session = boto3.Session()  # For private bucket\n",
    "\n",
    "# Clients for respective sessions\n",
    "openaq_client = anonymous_session.client('s3', region_name=REGION_NAME, config=Config(signature_version=UNSIGNED))\n",
    "target_s3_client = authenticated_session.client('s3', region_name=REGION_NAME)\n",
    "\n",
    "headers = {\"X-API-Key\": API_KEY}\n",
    "\n",
    "\n",
    "def load_city_config(config_file):\n",
    "    \"\"\"Loads city configuration from a JSON file.\"\"\"\n",
    "    with open(config_file, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def extract_metadata_fields(metadata):\n",
    "    \"\"\"\n",
    "    Extracts metadata fields required for enrichment, including country name, owner name, \n",
    "    provider name, instruments, isMobile, and isMonitor.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"location_id\": metadata.get(\"id\",\"\"),\n",
    "        \"location_name\": metadata.get(\"name\", \"\"),\n",
    "        \"country\": metadata.get(\"country\", {}).get(\"name\", \"\"),\n",
    "        \"owner\": metadata.get(\"owner\", {}).get(\"name\", \"\"),\n",
    "        \"provider\": metadata.get(\"provider\", {}).get(\"name\", \"\"),\n",
    "        \"instruments\": \", \".join(\n",
    "            [instrument.get(\"name\", \"\") for instrument in metadata.get(\"instruments\", [])]\n",
    "        ),\n",
    "        \"isMobile\": metadata.get(\"isMobile\", False),\n",
    "        \"isMonitor\": metadata.get(\"isMonitor\", False),\n",
    "    }\n",
    "\n",
    "\n",
    "def enrich_data_with_metadata(data_df, metadata):\n",
    "    \"\"\"Enriches the data DataFrame with additional metadata fields.\"\"\"\n",
    "    # Map metadata to DataFrame using the `location` column directly\n",
    "    data_df[\"location_name\"] = data_df[\"location_id\"].map(\n",
    "        lambda location_id: metadata.get(location_id, {}).get(\"location_name\", \"\")\n",
    "    )\n",
    "    data_df[\"country\"] = data_df[\"location_id\"].map(\n",
    "        lambda location_id: metadata.get(location_id, {}).get(\"country\", \"\")\n",
    "    )\n",
    "    data_df[\"owner\"] = data_df[\"location_id\"].map(\n",
    "        lambda location_id: metadata.get(location_id, {}).get(\"owner\", \"\")\n",
    "    )\n",
    "    data_df[\"provider\"] = data_df[\"location_id\"].map(\n",
    "        lambda location_id: metadata.get(location_id, {}).get(\"provider\", \"\")\n",
    "    )\n",
    "    data_df[\"instruments\"] = data_df[\"location_id\"].map(\n",
    "        lambda location_id: metadata.get(location_id, {}).get(\"instruments\", \"\")\n",
    "    )\n",
    "    data_df[\"isMobile\"] = data_df[\"location_id\"].map(\n",
    "        lambda location_id: metadata.get(location_id, {}).get(\"isMobile\", False)\n",
    "    )\n",
    "    data_df[\"isMonitor\"] = data_df[\"location_id\"].map(\n",
    "        lambda location_id: metadata.get(location_id, {}).get(\"isMonitor\", False)\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        data_df[\n",
    "            [\"location\", \"location_name\", \"country\", \"owner\", \"provider\", \"instruments\", \"isMobile\", \"isMonitor\"]\n",
    "        ].head()\n",
    "    )  # Debugging\n",
    "    return data_df\n",
    "\n",
    "\n",
    "def get_location_metadata(city, bbox, api_key):\n",
    "    \"\"\"\n",
    "    Fetch metadata for all locations within a city's bounding box.\n",
    "    \"\"\"\n",
    "    url = \"https://api.openaq.org/v3/locations\"\n",
    "    headers = {\"X-API-Key\": api_key}\n",
    "    params = {\"bbox\": bbox, \"limit\": 1000}\n",
    "\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    response.raise_for_status()\n",
    "    location_data = response.json().get(\"results\", [])\n",
    "    \n",
    "    metadata = {}\n",
    "    for location in location_data:\n",
    "        location_id = location[\"id\"]  # Ensure location ID is a string\n",
    "        metadata[location_id] = extract_metadata_fields(location)\n",
    "\n",
    "    print(f\"Fetched metadata for {len(metadata)} locations in {city}.\")  # Debugging\n",
    "    return metadata\n",
    "\n",
    "\n",
    "\n",
    "def get_location_ids(city, bbox):\n",
    "    \"\"\"Fetches location IDs for a city using the OpenAQ API.\"\"\"\n",
    "    locations_url = \"https://api.openaq.org/v3/locations\"\n",
    "    locations_params = {\"bbox\": bbox, \"limit\": 1000}\n",
    "    response = requests.get(locations_url, params=locations_params, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    results = response.json().get(\"results\", [])\n",
    "    return [location[\"id\"] for location in results]\n",
    "\n",
    "def generate_date_range(start_date, end_date):\n",
    "    \"\"\"Generates a list of dates between start_date and end_date.\"\"\"\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        yield current_date\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "def download_data_to_dataframe(location_ids, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Downloads data for the given location IDs and consolidates it into a single DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        location_ids (list): List of location IDs to fetch data for.\n",
    "        start_date (datetime): Start date for fetching data.\n",
    "        end_date (datetime): End date for fetching data.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Consolidated DataFrame with data for all locations.\n",
    "    \"\"\"\n",
    "    consolidated_df = pd.DataFrame()\n",
    "    failed_locations = []  # To track locations that fail to return data\n",
    "    \n",
    "    for location_id in location_ids:\n",
    "        for date in generate_date_range(start_date, end_date):\n",
    "            year, month, day = date.strftime(\"%Y\"), date.strftime(\"%m\"), date.strftime(\"%d\")\n",
    "            prefix = f\"records/csv.gz/locationid={location_id}/year={year}/month={month}/\"\n",
    "            response = openaq_client.list_objects_v2(Bucket=SOURCE_BUCKET_NAME, Prefix=prefix)\n",
    "\n",
    "            if 'Contents' in response:\n",
    "                for obj in response['Contents']:\n",
    "                    key = obj['Key']\n",
    "                    if key.endswith(f\"{year}{month}{day}.csv.gz\"):\n",
    "#                        print(f\"Processing file: {key}\")\n",
    "                        # Download and process the file\n",
    "                        obj = openaq_client.get_object(Bucket=SOURCE_BUCKET_NAME, Key=key)\n",
    "                        with gzip.GzipFile(fileobj=obj['Body']) as gz_file:\n",
    "                            daily_df = pd.read_csv(gz_file)\n",
    "                            consolidated_df = pd.concat([consolidated_df, daily_df], ignore_index=True)\n",
    "            else:\n",
    "                # print(f\"No data returned for location ID: {location_id}\")\n",
    "                failed_locations.append(location_id)             \n",
    "\n",
    "    \n",
    "    if not consolidated_df.empty:\n",
    "        print(f\"Data successfully consolidated. Total records: {len(consolidated_df)}\")\n",
    "    else:\n",
    "        print(\"No data was fetched for any location IDs.\")\n",
    "    \n",
    "    if failed_locations:\n",
    "        print(f\"Locations with no data or errors: {failed_locations}\")\n",
    "    \n",
    "    return consolidated_df\n",
    "\n",
    "\n",
    "    return consolidated_df\n",
    "\n",
    "def transform_data_for_rag(df):\n",
    "    \"\"\"Prepares data for RAG application.\"\"\"\n",
    "    # Normalize column names\n",
    "    df.columns = [col.lower().replace(\" \", \"_\") for col in df.columns]\n",
    "\n",
    "    # Add metadata (e.g., location, timestamp)\n",
    "    if 'location' not in df.columns:\n",
    "        df['location'] = \"Unknown\"  # Placeholder if location is missing\n",
    "    if 'date' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "\n",
    "    # Remove duplicates and null values\n",
    "    df = df.drop_duplicates().dropna()\n",
    "\n",
    "    # Add unique identifier for vector DB\n",
    "    df['record_id'] = range(1, len(df) + 1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def save_to_s3(df, bucket_name, file_key):\n",
    "    \"\"\"Saves a DataFrame to an S3 bucket as a CSV file.\"\"\"\n",
    "    csv_data = df.to_csv(index=False)\n",
    "    target_s3_client.put_object(Bucket=bucket_name, Key=file_key, Body=csv_data)\n",
    "    print(f\"Consolidated data saved to S3 at: s3://{bucket_name}/{file_key}\")\n",
    "\n",
    "# # Main execution\n",
    "# def main():\n",
    "#     city_config = load_city_config(CITIES_CONFIG_FILE)\n",
    "\n",
    "#     start_date = datetime(2022, 5, 1)  # Specify start date\n",
    "#     end_date = datetime(2022, 5, 7)    # Specify end date\n",
    "\n",
    "#     all_cities_data = pd.DataFrame()\n",
    "\n",
    "#     for city, bbox in city_config.items():\n",
    "#         print(f\"Fetching location IDs for city: {city}\")\n",
    "#         location_ids = get_location_ids(city, bbox)\n",
    "\n",
    "#         if location_ids:\n",
    "#             print(f\"Found {len(location_ids)} locations for {city}. Downloading data...\")\n",
    "#             city_data = download_data_to_dataframe(location_ids, start_date, end_date)\n",
    "#             all_cities_data = pd.concat([all_cities_data, city_data], ignore_index=True)\n",
    "\n",
    "#     if not all_cities_data.empty:\n",
    "#         # Transform data for RAG\n",
    "#         transformed_df = transform_data_for_rag(all_cities_data)\n",
    "\n",
    "#         # Save to S3\n",
    "#         save_to_s3(transformed_df, TARGET_BUCKET_NAME, OUTPUT_FILE_KEY)\n",
    "#     else:\n",
    "#         print(\"No data found for any city in the specified date range.\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching location metadata for city: Chennai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched metadata for 9 locations in Chennai.\n",
      "Fetching location IDs for city: Chennai\n",
      "Found 9 locations for Chennai. Downloading data...\n",
      "Data successfully consolidated. Total records: 253\n",
      "Locations with no data or errors: [378, 378, 378, 378, 378, 378, 378, 378, 378, 378, 378, 2461, 2461, 2461, 2461, 2461, 2461, 2461, 2461, 2461, 2461, 2461, 2549, 2549, 2549, 2549, 2549, 2549, 2549, 2549, 2549, 2549, 2549, 5655, 5655, 5655, 5655, 5655, 5655, 5655, 5655, 5655, 5655, 5655, 11578, 11578, 11578, 11578, 11578, 11578, 11578, 11578, 11578, 11578, 11578, 11579, 11579, 11579, 11579, 11579, 11579, 11579, 11579, 11579, 11579, 11579, 11581, 11581, 11581, 11581, 11581, 11581, 11581, 11581, 11581, 11581, 11581, 12046, 12046, 12046, 12046, 12046, 12046, 12046, 12046, 12046, 12046, 12046]\n",
      "         location location_name country                              owner  \\\n",
      "0  Chennai-931708       Chennai   India  Unknown Governmental Organization   \n",
      "1  Chennai-931708       Chennai   India  Unknown Governmental Organization   \n",
      "2  Chennai-931708       Chennai   India  Unknown Governmental Organization   \n",
      "3  Chennai-931708       Chennai   India  Unknown Governmental Organization   \n",
      "4  Chennai-931708       Chennai   India  Unknown Governmental Organization   \n",
      "\n",
      "  provider         instruments  isMobile  isMonitor  \n",
      "0   AirNow  Government Monitor     False       True  \n",
      "1   AirNow  Government Monitor     False       True  \n",
      "2   AirNow  Government Monitor     False       True  \n",
      "3   AirNow  Government Monitor     False       True  \n",
      "4   AirNow  Government Monitor     False       True  \n",
      "Consolidated data saved to S3 at: s3://go01-demo/user/vishr/data/airquality/Chennai_data.csv\n",
      "Data for city Chennai saved to user/vishr/data/airquality/Chennai_data.csv in S3.\n",
      "Processing completed for all cities.\n"
     ]
    }
   ],
   "source": [
    "END_DATE_STRING = \"31/12/2023 23:59:59 +0530\"  #Enter the end date (YYYY-MM-DD):\n",
    "NUMBER_OF_DAYS= 10\n",
    "CITIES_CONFIG_FILE=\"cities_config.json\"\n",
    "end_date = datetime.strptime(str(END_DATE_STRING), \"%d/%m/%Y %H:%M:%S %z\")\n",
    "start_date = end_date - timedelta(days=NUMBER_OF_DAYS)\n",
    "city_config = load_city_config(CITIES_CONFIG_FILE)\n",
    "for city, bbox in city_config.items():\n",
    "    print(f\"Fetching location metadata for city: {city}\")\n",
    "    location_metadata = get_location_metadata(city, bbox, API_KEY)\n",
    "\n",
    "    print(f\"Fetching location IDs for city: {city}\")\n",
    "    location_ids = list(location_metadata.keys())\n",
    "\n",
    "    if location_ids:\n",
    "        print(f\"Found {len(location_ids)} locations for {city}. Downloading data...\")\n",
    "        city_data = download_data_to_dataframe(location_ids, start_date, end_date)\n",
    "        \n",
    "        if not city_data.empty:\n",
    "            # Enrich with metadata only if city_data is not empty\n",
    "            city_data = enrich_data_with_metadata(city_data, location_metadata)\n",
    "            \n",
    "            # Transform data for RAG\n",
    "            transformed_df = transform_data_for_rag(city_data)\n",
    "            \n",
    "            # Define unique output file for each city\n",
    "            city_output_key = f\"{OUTPUT_FILE_KEY_PREFIX}/{city}_data.csv\"\n",
    "            \n",
    "            # Save the city's transformed data to S3\n",
    "            save_to_s3(transformed_df, TARGET_BUCKET_NAME, city_output_key)\n",
    "            print(f\"Data for city {city} saved to {city_output_key} in S3.\")\n",
    "        else:\n",
    "            print(f\"No data available for city: {city} within the specified date range.\")\n",
    "    else:\n",
    "        print(f\"No locations found for city: {city}. Skipping data download.\")\n",
    "\n",
    "print(\"Processing completed for all cities.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
