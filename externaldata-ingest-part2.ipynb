{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This Notebook imports the Airquality Data from an external S3 File system and saves it to a Cloudera Data lake\n",
    "As a variant, in this approach we use Spark to do data ingest and transformation. Since this will be used subsequently as a GenAI system input, we are also doing some transformations that will help with Retrieval Augmented Generation based systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/cdsw\")\n",
    "from utils.access_keys import ACCESS_KEY, SECRET_KEY, SESSION_TOKEN, REGION_NAME\n",
    "import boto3\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "CITIES_CONFIG_FILE =  os.environ[\"CITIES_CONFIG_FILE\"]\n",
    "SOURCE_BUCKET_NAME = os.environ[\"OPENAQ_DATA_SOURCE\"] #  \"openaq-data-archive\"\n",
    "TARGET_BUCKET_NAME = os.environ[\"TARGET_BUCKET_NAME\"] # Replace with your destination bucket\n",
    "OUTPUT_FILE_KEY_PREFIX =  os.environ[\"OUTPUT_FILE_KEY_PREFIX\"]  # \"/user/vishr/data/airquality\"\n",
    "API_KEY = os.environ[\"API_KEY\"]  #= # Replace with your actual OpenAQ API key\n",
    "# Initialize sessions\n",
    "anonymous_session = boto3.Session()  # For public bucket\n",
    "authenticated_session = boto3.Session()  # For private bucket\n",
    "\n",
    "# Clients for respective sessions\n",
    "openaq_client = anonymous_session.client('s3', region_name=REGION_NAME, config=Config(signature_version=UNSIGNED))\n",
    "target_s3_client = authenticated_session.client('s3', region_name=REGION_NAME)\n",
    "\n",
    "headers = {\"X-API-Key\": API_KEY}\n",
    "\n",
    "\n",
    "def load_city_config(config_file):\n",
    "    \"\"\"Loads city configuration from a JSON file.\"\"\"\n",
    "    with open(config_file, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def extract_metadata_fields(metadata):\n",
    "    \"\"\"\n",
    "    Extracts metadata fields required for enrichment, including country name, owner name, \n",
    "    provider name, instruments, isMobile, and isMonitor.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"location_id\": metadata.get(\"id\",\"\"),\n",
    "        \"location_name\": metadata.get(\"name\", \"\"),\n",
    "        \"country\": metadata.get(\"country\", {}).get(\"name\", \"\"),\n",
    "        \"owner\": metadata.get(\"owner\", {}).get(\"name\", \"\"),\n",
    "        \"provider\": metadata.get(\"provider\", {}).get(\"name\", \"\"),\n",
    "        \"instruments\": \", \".join(\n",
    "            [instrument.get(\"name\", \"\") for instrument in metadata.get(\"instruments\", [])]\n",
    "        ),\n",
    "        \"isMobile\": metadata.get(\"isMobile\", False),\n",
    "        \"isMonitor\": metadata.get(\"isMonitor\", False),\n",
    "    }\n",
    "\n",
    "\n",
    "def enrich_data_with_metadata(data_df, metadata):\n",
    "    \"\"\"Enriches the data DataFrame with additional metadata fields.\"\"\"\n",
    "    # Map metadata to DataFrame using the `location` column directly\n",
    "    data_df[\"location_name\"] = data_df[\"location_id\"].map(\n",
    "        lambda location_id: metadata.get(location_id, {}).get(\"location_name\", \"\")\n",
    "    )\n",
    "    data_df[\"country\"] = data_df[\"location_id\"].map(\n",
    "        lambda location_id: metadata.get(location_id, {}).get(\"country\", \"\")\n",
    "    )\n",
    "    data_df[\"owner\"] = data_df[\"location_id\"].map(\n",
    "        lambda location_id: metadata.get(location_id, {}).get(\"owner\", \"\")\n",
    "    )\n",
    "    data_df[\"provider\"] = data_df[\"location_id\"].map(\n",
    "        lambda location_id: metadata.get(location_id, {}).get(\"provider\", \"\")\n",
    "    )\n",
    "    data_df[\"instruments\"] = data_df[\"location_id\"].map(\n",
    "        lambda location_id: metadata.get(location_id, {}).get(\"instruments\", \"\")\n",
    "    )\n",
    "    data_df[\"isMobile\"] = data_df[\"location_id\"].map(\n",
    "        lambda location_id: metadata.get(location_id, {}).get(\"isMobile\", False)\n",
    "    )\n",
    "    data_df[\"isMonitor\"] = data_df[\"location_id\"].map(\n",
    "        lambda location_id: metadata.get(location_id, {}).get(\"isMonitor\", False)\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        data_df[\n",
    "            [\"location\", \"location_name\", \"country\", \"owner\", \"provider\", \"instruments\", \"isMobile\", \"isMonitor\"]\n",
    "        ].head()\n",
    "    )  # Debugging\n",
    "    return data_df\n",
    "\n",
    "\n",
    "def get_location_metadata(city, bbox, api_key):\n",
    "    \"\"\"\n",
    "    Fetch metadata for all locations within a city's bounding box.\n",
    "    \"\"\"\n",
    "    url = \"https://api.openaq.org/v3/locations\"\n",
    "    headers = {\"X-API-Key\": api_key}\n",
    "    params = {\"bbox\": bbox, \"limit\": 1000}\n",
    "\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    response.raise_for_status()\n",
    "    location_data = response.json().get(\"results\", [])\n",
    "    \n",
    "    metadata = {}\n",
    "    for location in location_data:\n",
    "        location_id = location[\"id\"]  # Ensure location ID is a string\n",
    "        metadata[location_id] = extract_metadata_fields(location)\n",
    "\n",
    "    print(f\"Fetched metadata for {len(metadata)} locations in {city}.\")  # Debugging\n",
    "    return metadata\n",
    "\n",
    "\n",
    "\n",
    "def get_location_ids(city, bbox):\n",
    "    \"\"\"Fetches location IDs for a city using the OpenAQ API.\"\"\"\n",
    "    locations_url = \"https://api.openaq.org/v3/locations\"\n",
    "    locations_params = {\"bbox\": bbox, \"limit\": 1000}\n",
    "    response = requests.get(locations_url, params=locations_params, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    results = response.json().get(\"results\", [])\n",
    "    return [location[\"id\"] for location in results]\n",
    "\n",
    "def generate_date_range(start_date, end_date):\n",
    "    \"\"\"Generates a list of dates between start_date and end_date.\"\"\"\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        yield current_date\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "def download_data_to_dataframe(location_ids, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Downloads data for the given location IDs and consolidates it into a single DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        location_ids (list): List of location IDs to fetch data for.\n",
    "        start_date (datetime): Start date for fetching data.\n",
    "        end_date (datetime): End date for fetching data.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Consolidated DataFrame with data for all locations.\n",
    "    \"\"\"\n",
    "    consolidated_df = pd.DataFrame()\n",
    "    failed_locations = []  # To track locations that fail to return data\n",
    "    \n",
    "    for location_id in location_ids:\n",
    "        for date in generate_date_range(start_date, end_date):\n",
    "            year, month, day = date.strftime(\"%Y\"), date.strftime(\"%m\"), date.strftime(\"%d\")\n",
    "            prefix = f\"records/csv.gz/locationid={location_id}/year={year}/month={month}/\"\n",
    "            response = openaq_client.list_objects_v2(Bucket=SOURCE_BUCKET_NAME, Prefix=prefix)\n",
    "\n",
    "            if 'Contents' in response:\n",
    "                for obj in response['Contents']:\n",
    "                    key = obj['Key']\n",
    "                    if key.endswith(f\"{year}{month}{day}.csv.gz\"):\n",
    "#                        print(f\"Processing file: {key}\")\n",
    "                        # Download and process the file\n",
    "                        obj = openaq_client.get_object(Bucket=SOURCE_BUCKET_NAME, Key=key)\n",
    "                        with gzip.GzipFile(fileobj=obj['Body']) as gz_file:\n",
    "                            daily_df = pd.read_csv(gz_file)\n",
    "                            consolidated_df = pd.concat([consolidated_df, daily_df], ignore_index=True)\n",
    "            else:\n",
    "                # print(f\"No data returned for location ID: {location_id}\")\n",
    "                failed_locations.append(location_id)             \n",
    "\n",
    "    \n",
    "    if not consolidated_df.empty:\n",
    "        print(f\"Data successfully consolidated. Total records: {len(consolidated_df)}\")\n",
    "    else:\n",
    "        print(\"No data was fetched for any location IDs.\")\n",
    "    \n",
    "    if failed_locations:\n",
    "        print(f\"Locations with no data or errors: {failed_locations}\")\n",
    "    \n",
    "    return consolidated_df\n",
    "\n",
    "\n",
    "    return consolidated_df\n",
    "\n",
    "def transform_data_for_rag(df):\n",
    "    \"\"\"Prepares data for RAG application.\"\"\"\n",
    "    # Normalize column names\n",
    "    df.columns = [col.lower().replace(\" \", \"_\") for col in df.columns]\n",
    "\n",
    "    # Add metadata (e.g., location, timestamp)\n",
    "    if 'location' not in df.columns:\n",
    "        df['location'] = \"Unknown\"  # Placeholder if location is missing\n",
    "    if 'date' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "\n",
    "    # Remove duplicates and null values\n",
    "    df = df.drop_duplicates().dropna()\n",
    "\n",
    "    # Add unique identifier for vector DB\n",
    "    df['record_id'] = range(1, len(df) + 1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def save_to_s3(df, bucket_name, file_key):\n",
    "    \"\"\"Saves a DataFrame to an S3 bucket as a CSV file.\"\"\"\n",
    "    csv_data = df.to_csv(index=False)\n",
    "    target_s3_client.put_object(Bucket=bucket_name, Key=file_key, Body=csv_data)\n",
    "    print(f\"Consolidated data saved to S3 at: s3://{bucket_name}/{file_key}\")\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    END_DATE_STRING = os.getenv(\"END_DATE\",  \"31/12/2023 23:59:59 +0530\")  #Enter the end date (YYYY-MM-DD):\n",
    "    NUMBER_OF_DAYS= int(os.getenv(\"NUMBER_OF_DAYS\", 10))\n",
    "    CITIES_CONFIG_FILE=os.getenv(\"CITIES_CONFIG_FILE\", \"cities_config.json\")\n",
    "    end_date = datetime.strptime(str(END_DATE_STRING), \"%d/%m/%Y %H:%M:%S %z\")\n",
    "    start_date = end_date - timedelta(days=NUMBER_OF_DAYS)\n",
    "    city_config = load_city_config(CITIES_CONFIG_FILE)\n",
    "    for city, bbox in city_config.items():\n",
    "        print(f\"Fetching location metadata for city: {city}\")\n",
    "        location_metadata = get_location_metadata(city, bbox, API_KEY)\n",
    "\n",
    "        print(f\"Fetching location IDs for city: {city}\")\n",
    "        location_ids = list(location_metadata.keys())\n",
    "\n",
    "        if location_ids:\n",
    "            print(f\"Found {len(location_ids)} locations for {city}. Downloading data...\")\n",
    "            city_data = download_data_to_dataframe(location_ids, start_date, end_date)\n",
    "            \n",
    "            if not city_data.empty:\n",
    "                # Enrich with metadata only if city_data is not empty\n",
    "                city_data = enrich_data_with_metadata(city_data, location_metadata)\n",
    "                \n",
    "                # Transform data for RAG\n",
    "                transformed_df = transform_data_for_rag(city_data)\n",
    "                \n",
    "                # Define unique output file for each city\n",
    "                city_output_key = f\"{OUTPUT_FILE_KEY_PREFIX}/{city}_data.csv\"\n",
    "                \n",
    "                # Save the city's transformed data to S3\n",
    "                save_to_s3(transformed_df, TARGET_BUCKET_NAME, city_output_key)\n",
    "                print(f\"Data for city {city} saved to {city_output_key} in S3.\")\n",
    "            else:\n",
    "                print(f\"No data available for city: {city} within the specified date range.\")\n",
    "        else:\n",
    "            print(f\"No locations found for city: {city}. Skipping data download.\")\n",
    "\n",
    "    print(\"Processing completed for all cities.\")\n",
    "\n",
    "# uncomment this if you are using this inside .py file\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "\n",
    "# comment the below if you are using a .py file\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
